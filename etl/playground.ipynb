{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aqi</th>\n",
       "      <th>co</th>\n",
       "      <th>datetime</th>\n",
       "      <th>no2</th>\n",
       "      <th>o3</th>\n",
       "      <th>pm10</th>\n",
       "      <th>pm25</th>\n",
       "      <th>so2</th>\n",
       "      <th>timestamp_local</th>\n",
       "      <th>timestamp_utc</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>157</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2023-11-17:09</td>\n",
       "      <td>5.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2023-11-17T16:00:00</td>\n",
       "      <td>2023-11-17T09:00:00</td>\n",
       "      <td>1700211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104</td>\n",
       "      <td>185.7</td>\n",
       "      <td>2023-11-17:08</td>\n",
       "      <td>4.0</td>\n",
       "      <td>125.7</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.3</td>\n",
       "      <td>2023-11-17T15:00:00</td>\n",
       "      <td>2023-11-17T08:00:00</td>\n",
       "      <td>1700208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128</td>\n",
       "      <td>194.3</td>\n",
       "      <td>2023-11-17:07</td>\n",
       "      <td>3.0</td>\n",
       "      <td>139.3</td>\n",
       "      <td>45.7</td>\n",
       "      <td>45.67</td>\n",
       "      <td>43.7</td>\n",
       "      <td>2023-11-17T14:00:00</td>\n",
       "      <td>2023-11-17T07:00:00</td>\n",
       "      <td>1700204400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107</td>\n",
       "      <td>203.0</td>\n",
       "      <td>2023-11-17:06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2023-11-17T13:00:00</td>\n",
       "      <td>2023-11-17T06:00:00</td>\n",
       "      <td>1700200800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137</td>\n",
       "      <td>223.7</td>\n",
       "      <td>2023-11-17:05</td>\n",
       "      <td>2.7</td>\n",
       "      <td>149.3</td>\n",
       "      <td>49.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2023-11-17T12:00:00</td>\n",
       "      <td>2023-11-17T05:00:00</td>\n",
       "      <td>1700197200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aqi     co       datetime  no2     o3  pm10   pm25   so2  \\\n",
       "0  157  177.0  2023-11-17:09  5.0  112.0  61.0   61.0  31.0   \n",
       "1  104  185.7  2023-11-17:08  4.0  125.7  37.0   37.0  37.3   \n",
       "2  128  194.3  2023-11-17:07  3.0  139.3  45.7  45.67  43.7   \n",
       "3  107  203.0  2023-11-17:06  2.0  153.0  38.0   38.0  50.0   \n",
       "4  137  223.7  2023-11-17:05  2.7  149.3  49.0   49.0  57.0   \n",
       "\n",
       "       timestamp_local        timestamp_utc          ts  \n",
       "0  2023-11-17T16:00:00  2023-11-17T09:00:00  1700211600  \n",
       "1  2023-11-17T15:00:00  2023-11-17T08:00:00  1700208000  \n",
       "2  2023-11-17T14:00:00  2023-11-17T07:00:00  1700204400  \n",
       "3  2023-11-17T13:00:00  2023-11-17T06:00:00  1700200800  \n",
       "4  2023-11-17T12:00:00  2023-11-17T05:00:00  1700197200  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../raw_data/air_quality_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jam_id</th>\n",
       "      <th>type</th>\n",
       "      <th>level</th>\n",
       "      <th>severity</th>\n",
       "      <th>line_coordinates</th>\n",
       "      <th>start_location</th>\n",
       "      <th>end_location</th>\n",
       "      <th>speed_kmh</th>\n",
       "      <th>length_meters</th>\n",
       "      <th>delay_seconds</th>\n",
       "      <th>block_alert_id</th>\n",
       "      <th>block_alert_type</th>\n",
       "      <th>block_alert_description</th>\n",
       "      <th>block_alert_update_datetime_utc</th>\n",
       "      <th>block_start_datetime_utc</th>\n",
       "      <th>publish_datetime_utc</th>\n",
       "      <th>update_datetime_utc</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>street</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1306709498</td>\n",
       "      <td>NONE</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'lat': -6.3498, 'lon': 106.749779}, {'lat': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.06</td>\n",
       "      <td>1263</td>\n",
       "      <td>111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-17T09:03:41.328Z</td>\n",
       "      <td>2023-11-17T10:30:24.539Z</td>\n",
       "      <td>ID</td>\n",
       "      <td>Tangerang Selatan</td>\n",
       "      <td>N6 RE Martadinata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1310361102</td>\n",
       "      <td>NONE</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'lat': -6.123958, 'lon': 106.704834}, {'lat'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.77</td>\n",
       "      <td>278</td>\n",
       "      <td>177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-17T09:55:00.340Z</td>\n",
       "      <td>2023-11-17T10:30:25.000Z</td>\n",
       "      <td>ID</td>\n",
       "      <td>Jakarta Barat</td>\n",
       "      <td>Citra Garden 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1312734760</td>\n",
       "      <td>NONE</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'lat': -6.149167, 'lon': 106.847985}, {'lat'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.41</td>\n",
       "      <td>347</td>\n",
       "      <td>109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-17T10:21:19.094Z</td>\n",
       "      <td>2023-11-17T10:30:27.341Z</td>\n",
       "      <td>ID</td>\n",
       "      <td>Jakarta Pusat</td>\n",
       "      <td>Kawasan PRJ Kemayoran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1312796200</td>\n",
       "      <td>NONE</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'lat': -6.305742, 'lon': 106.859124}, {'lat'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.33</td>\n",
       "      <td>373</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-17T10:21:18.419Z</td>\n",
       "      <td>2023-11-17T10:30:26.700Z</td>\n",
       "      <td>ID</td>\n",
       "      <td>Jakarta Timur</td>\n",
       "      <td>Kesehatan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1313179214</td>\n",
       "      <td>NONE</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'lat': -6.204239, 'lon': 106.872368}, {'lat'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.87</td>\n",
       "      <td>610</td>\n",
       "      <td>84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-17T10:25:11.798Z</td>\n",
       "      <td>2023-11-17T10:30:24.649Z</td>\n",
       "      <td>ID</td>\n",
       "      <td>Jakarta Timur</td>\n",
       "      <td>Kramat Asem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       jam_id  type level severity  \\\n",
       "0  1306709498  NONE     2        5   \n",
       "1  1310361102  NONE     4        5   \n",
       "2  1312734760  NONE     3        5   \n",
       "3  1312796200  NONE     3        5   \n",
       "4  1313179214  NONE     2        5   \n",
       "\n",
       "                                    line_coordinates start_location  \\\n",
       "0  [{'lat': -6.3498, 'lon': 106.749779}, {'lat': ...            NaN   \n",
       "1  [{'lat': -6.123958, 'lon': 106.704834}, {'lat'...            NaN   \n",
       "2  [{'lat': -6.149167, 'lon': 106.847985}, {'lat'...            NaN   \n",
       "3  [{'lat': -6.305742, 'lon': 106.859124}, {'lat'...            NaN   \n",
       "4  [{'lat': -6.204239, 'lon': 106.872368}, {'lat'...            NaN   \n",
       "\n",
       "  end_location speed_kmh length_meters delay_seconds block_alert_id  \\\n",
       "0          NaN     21.06          1263           111            NaN   \n",
       "1          NaN      4.77           278           177            NaN   \n",
       "2          NaN      8.41           347           109            NaN   \n",
       "3          NaN      5.33           373           187            NaN   \n",
       "4          NaN     12.87           610            84            NaN   \n",
       "\n",
       "  block_alert_type block_alert_description block_alert_update_datetime_utc  \\\n",
       "0              NaN                     NaN                             NaN   \n",
       "1              NaN                     NaN                             NaN   \n",
       "2              NaN                     NaN                             NaN   \n",
       "3              NaN                     NaN                             NaN   \n",
       "4              NaN                     NaN                             NaN   \n",
       "\n",
       "  block_start_datetime_utc      publish_datetime_utc  \\\n",
       "0                      NaN  2023-11-17T09:03:41.328Z   \n",
       "1                      NaN  2023-11-17T09:55:00.340Z   \n",
       "2                      NaN  2023-11-17T10:21:19.094Z   \n",
       "3                      NaN  2023-11-17T10:21:18.419Z   \n",
       "4                      NaN  2023-11-17T10:25:11.798Z   \n",
       "\n",
       "        update_datetime_utc country               city                 street  \n",
       "0  2023-11-17T10:30:24.539Z      ID  Tangerang Selatan      N6 RE Martadinata  \n",
       "1  2023-11-17T10:30:25.000Z      ID      Jakarta Barat         Citra Garden 5  \n",
       "2  2023-11-17T10:30:27.341Z      ID      Jakarta Pusat  Kawasan PRJ Kemayoran  \n",
       "3  2023-11-17T10:30:26.700Z      ID      Jakarta Timur              Kesehatan  \n",
       "4  2023-11-17T10:30:24.649Z      ID      Jakarta Timur            Kramat Asem  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../raw_data/traffic_jam_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do Transformation\n",
    "\n",
    "# Air Condition\n",
    "\n",
    "# Column to drop = datetime,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aqi: integer (nullable = true)\n",
      " |-- co: double (nullable = true)\n",
      " |-- no2: double (nullable = true)\n",
      " |-- o3: double (nullable = true)\n",
      " |-- pm10: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- so2: double (nullable = true)\n",
      " |-- timestamp_utc: timestamp (nullable = true)\n",
      "\n",
      "+---+-----+----+-----+----+-----+----+-------------------+\n",
      "|aqi|   co| no2|   o3|pm10| pm25| so2|      timestamp_utc|\n",
      "+---+-----+----+-----+----+-----+----+-------------------+\n",
      "| 94|265.0| 8.7|113.7|47.0|32.33|37.7|2023-11-14 10:00:00|\n",
      "|103|339.0|13.3| 95.3|53.0|36.67|39.3|2023-11-14 11:00:00|\n",
      "|115|413.0|18.0| 77.0|59.0| 41.0|41.0|2023-11-14 12:00:00|\n",
      "|118|420.0|16.7| 72.7|60.7| 42.0|38.3|2023-11-14 13:00:00|\n",
      "|121|427.0|15.3| 68.3|62.3| 43.0|35.7|2023-11-14 14:00:00|\n",
      "|123|434.0|14.0| 64.0|64.0| 44.0|33.0|2023-11-14 15:00:00|\n",
      "|130|419.7|14.3| 62.3|67.7|46.67|38.7|2023-11-14 16:00:00|\n",
      "|138|405.3|14.7| 60.7|71.3|49.33|44.3|2023-11-14 17:00:00|\n",
      "|145|391.0|15.0| 59.0|75.0| 52.0|50.0|2023-11-14 18:00:00|\n",
      "|148|426.7|16.0| 53.0|76.0| 53.0|53.7|2023-11-14 19:00:00|\n",
      "|150|462.3|17.0| 47.0|77.0| 54.0|57.3|2023-11-14 20:00:00|\n",
      "|153|498.0|18.0| 41.0|78.0| 55.0|61.0|2023-11-14 21:00:00|\n",
      "|153|565.0|19.0| 50.0|81.7|57.33|63.3|2023-11-14 22:00:00|\n",
      "|156|632.0|20.0| 59.0|85.3|59.67|65.7|2023-11-14 23:00:00|\n",
      "|159|699.0|21.0| 68.0|89.0| 62.0|68.0|2023-11-15 00:00:00|\n",
      "|153|584.0|15.7|109.7|83.3| 58.0|75.3|2023-11-15 01:00:00|\n",
      "|150|469.0|10.3|151.3|77.7| 54.0|82.7|2023-11-15 02:00:00|\n",
      "|139|354.0| 5.0|193.0|72.0| 50.0|90.0|2023-11-15 03:00:00|\n",
      "|130|331.3| 4.3|196.7|67.3|46.67|81.3|2023-11-15 04:00:00|\n",
      "|121|308.7| 3.7|200.3|62.7|43.33|72.7|2023-11-15 05:00:00|\n",
      "+---+-----+----+-----+----+-----+----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "['aqi', 'co', 'no2', 'o3', 'pm10', 'pm25', 'so2', 'timestamp_utc']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, when, lit, lag, lead\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CSV Reader\").getOrCreate()\n",
    "\n",
    "# Read CSV file into a Spark DataFrame\n",
    "file_path = \"../raw_data/traffic_jam_data.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "def replace_outliers_with_mean_adjacent(df, max_values):\n",
    "    windowSpec = Window.orderBy('timestamp_utc')  # Define your timestamp column here\n",
    "\n",
    "    for col_name in max_values.keys():\n",
    "        max_val = max_values.get(col_name, float('inf'))\n",
    "        df = df.withColumn(col_name,\n",
    "                           when((col(col_name) < 0) | (col(col_name) > max_val),\n",
    "                                  (lag(col_name, default=0).over(windowSpec)+ lead(col_name, default=0).over(windowSpec)) / 2.0)\n",
    "                           .otherwise(col(col_name)))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Cleaning data\n",
    "# drop unecessary columns\n",
    "columns_to_drop = ['line_coordinates', \n",
    "                   \n",
    "                   'update_datetime_utc']\n",
    "df = df.drop(*columns_to_drop)\n",
    "\n",
    "# rename publish_datetime_utc to timestamp_utc\n",
    "df = df.withColumnRenamed(\"publish_datetime_utc\", \"timestamp_utc\")\n",
    "\n",
    "# drom row with all missing value\n",
    "df = df.dropna(how ='all')\n",
    "\n",
    "# drop duplicates\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# consistent dtypes\n",
    "dtypes = {\n",
    "    'jam_id': 'integer', \n",
    "    'type': 'string', \n",
    "    'level': 'integer',\n",
    "    'severity': 'integer', \n",
    "    'speed_kmh': 'double', \n",
    "    'length_meters': 'integer',\n",
    "    'delay_seconds': 'integer',\n",
    "    'start_location': 'string', \n",
    "    'end_location': 'string', \n",
    "    'block_alert_id': 'integer', \n",
    "    'block_alert_type': 'string',\n",
    "    'block_alert_description' : 'string',\n",
    "    'block_alert_update_datetime_utc': 'timestamp',\n",
    "    'block_start_datetime_utc' :'timestamp',\n",
    "    'timestamp_utc': 'timestamp',\n",
    "    'country': 'string',\n",
    "    'city': 'string',\n",
    "    'street': 'string'}\n",
    "\n",
    "def consistent_dtype(df, dtype):\n",
    "    for col_name in dtype.keys():\n",
    "        type = dtype.get(col_name)\n",
    "        df = df.withColumn(col_name, col(col_name).cast(type))\n",
    "    return df\n",
    "\n",
    "df = consistent_dtype(df, dtypes)\n",
    "\n",
    "# consistent datetime format\n",
    "df = df.withColumn('timestamp_utc', to_timestamp('timestamp_utc', 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# handle Disgusting Values\n",
    "max_val = {'co':150000.0, 'no2': 3840.0, 'o3': 1200.0, 'pm10': 600.0, 'pm25': 500.0, 'so2': 800.0}\n",
    "df = replace_outliers_with_mean_adjacent(df, max_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Show the DataFrame schema and preview the data\n",
    "df.printSchema()\n",
    "df.show()\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\siswa\\Documents\\KULIAH\\Data Engineer\\end-to-end-data-engineering\\etl\\playground.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/siswa/Documents/KULIAH/Data%20Engineer/end-to-end-data-engineering/etl/playground.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/siswa/Documents/KULIAH/Data%20Engineer/end-to-end-data-engineering/etl/playground.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Create a SparkSession\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/siswa/Documents/KULIAH/Data%20Engineer/end-to-end-data-engineering/etl/playground.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mCSV Reader\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/siswa/Documents/KULIAH/Data%20Engineer/end-to-end-data-engineering/etl/playground.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Read CSV file into a Spark DataFrame\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/siswa/Documents/KULIAH/Data%20Engineer/end-to-end-data-engineering/etl/playground.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../raw_data/traffic_jam_data.csv\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\siswa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:503\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    500\u001b[0m     session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n\u001b[0;32m    501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    502\u001b[0m     \u001b[39mgetattr\u001b[39m(\n\u001b[1;32m--> 503\u001b[0m         \u001b[39mgetattr\u001b[39m(session\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m     )\u001b[39m.\u001b[39mapplyModifiableSettings(session\u001b[39m.\u001b[39m_jsparkSession, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n\u001b[0;32m    505\u001b[0m \u001b[39mreturn\u001b[39;00m session\n",
      "File \u001b[1;32mc:\\Users\\siswa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1709\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m UserHelpAutoCompletion\u001b[39m.\u001b[39mKEY:\n\u001b[0;32m   1710\u001b[0m     \u001b[39mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[1;32m-> 1712\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client\u001b[39m.\u001b[39;49msend_command(\n\u001b[0;32m   1713\u001b[0m     proto\u001b[39m.\u001b[39;49mREFLECTION_COMMAND_NAME \u001b[39m+\u001b[39;49m\n\u001b[0;32m   1714\u001b[0m     proto\u001b[39m.\u001b[39;49mREFL_GET_UNKNOWN_SUB_COMMAND_NAME \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_id \u001b[39m+\u001b[39;49m\n\u001b[0;32m   1715\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m proto\u001b[39m.\u001b[39;49mEND_COMMAND_PART)\n\u001b[0;32m   1716\u001b[0m \u001b[39mif\u001b[39;00m answer \u001b[39m==\u001b[39m proto\u001b[39m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[0;32m   1717\u001b[0m     \u001b[39mreturn\u001b[39;00m JavaPackage(name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client, jvm_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id)\n",
      "File \u001b[1;32mc:\\Users\\siswa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[0;32m   1037\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mc:\\Users\\siswa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[0;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Users\\siswa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[0;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Users\\siswa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mconnect((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_port))\n\u001b[0;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CSV Reader\").getOrCreate()\n",
    "\n",
    "# Read CSV file into a Spark DataFrame\n",
    "file_path = \"../raw_data/traffic_jam_data.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame schema and preview the data\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+--------+--------------+------------+---------+-------------+-------------+--------------+-----------------+-----------------------+-------------------------------+------------------------+--------------------+--------------------+-------+-----------------+--------------------+\n",
      "|    jam_id|type|level|severity|start_location|end_location|speed_kmh|length_meters|delay_seconds|block_alert_id| block_alert_type|block_alert_description|block_alert_update_datetime_utc|block_start_datetime_utc|       timestamp_utc| update_datetime_utc|country|             city|              street|\n",
      "+----------+----+-----+--------+--------------+------------+---------+-------------+-------------+--------------+-----------------+-----------------------+-------------------------------+------------------------+--------------------+--------------------+-------+-----------------+--------------------+\n",
      "|1283271796|NONE|    5|       5|          NULL|        NULL|      0.0|          633|            0|    1823987321|ROAD_CLOSED_EVENT|                   NULL|           2023-10-10 15:26:...|     2023-10-10 15:26:00|2023-11-17 09:54:...|2023-11-17 17:34:...|     ID|    Jakarta Timur|Akses Tol Jakarta...|\n",
      "|1843133808|NONE|    5|       5|          NULL|        NULL|      0.0|          131|            0|    1133306783|ROAD_CLOSED_EVENT|   Konstruksi ~ 15 D...|           2023-08-13 18:01:...|     2023-07-29 01:00:00|2023-11-06 09:27:...|2023-11-19 10:49:...|     ID|    Jakarta Timur| Pondok Ranggon Raya|\n",
      "|1743271644|NONE|    5|       5|          NULL|        NULL|      0.0|          137|            0|    1845039622|ROAD_CLOSED_EVENT|             Konstruksi|           2023-10-12 13:03:...|     2023-10-12 13:03:00|2023-10-12 13:06:...|2023-11-19 10:49:...|     ID|    Jakarta Pusat|          Gajah Mada|\n",
      "|1107463206|NONE|    5|       5|          NULL|        NULL|      0.0|           99|            0|    1650243011|ROAD_CLOSED_EVENT|                   NULL|           2023-09-24 19:14:...|     2023-09-25 06:55:00|2023-11-16 09:18:...|2023-11-19 10:49:...|     ID|    Jakarta Barat|          Patra Raya|\n",
      "|1083856934|NONE|    5|       5|         Depok|       Depok|      0.0|          320|            0|    1725643606|ROAD_CLOSED_EVENT|                   NULL|           2023-10-01 06:51:...|     2023-10-01 06:51:00|2023-11-14 14:42:...|2023-11-19 10:49:...|     ID|            Depok|  Tol Serpong-Cinere|\n",
      "|1819080872|NONE|    5|       5|          NULL|        NULL|      0.0|          195|            0|    1747067436|ROAD_CLOSED_EVENT|             Konstruksi|           2023-10-03 11:40:...|     2023-10-03 11:38:00|2023-11-19 09:20:...|2023-11-19 10:49:...|     ID|            Depok|                NULL|\n",
      "|1819105408|NONE|    5|       5|          NULL|        NULL|      0.0|          335|            0|    1747065676|ROAD_CLOSED_EVENT|             Konstruksi|           2023-10-03 11:40:...|     2023-10-03 11:38:00|2023-11-19 09:21:...|2023-11-19 10:49:...|     ID|            Depok|                NULL|\n",
      "|1820891224|NONE|    5|       5|          NULL|        NULL|      0.0|           80|            0|     115920286|ROAD_CLOSED_EVENT|   Perbaikan  ~ Akhi...|           2023-11-19 10:27:...|     2023-11-19 10:20:00|2023-11-19 10:29:...|2023-11-19 10:49:...|     ID|            Depok|         Krukut Raya|\n",
      "|1820780552|NONE|    5|       5|          NULL|        NULL|      0.0|           41|            0|     115899183|ROAD_CLOSED_EVENT|   Perbaikan  ~ Akhi...|           2023-11-19 10:21:...|     2023-11-19 10:20:00|2023-11-19 10:23:...|2023-11-19 10:49:...|     ID|            Depok|         Krukut Raya|\n",
      "| 655975194|NONE|    5|       5|          NULL|        NULL|      0.0|           99|            0|     231921145|ROAD_CLOSED_EVENT|   Perbaikan Jembata...|           2023-11-07 16:28:...|     2023-11-07 16:25:00|2023-11-10 23:23:...|2023-11-19 10:49:...|     ID|Tangerang Selatan|      AMD 15-16 Raya|\n",
      "|1820931392|NONE|    5|       5|          NULL|        NULL|      0.0|           58|            0|     115920958|ROAD_CLOSED_EVENT|   Perbaikan  ~ Akhi...|           2023-11-19 10:27:...|     2023-11-19 10:20:00|2023-11-19 10:29:...|2023-11-19 10:49:...|     ID|            Depok|         Krukut Raya|\n",
      "|1820907920|NONE|    5|       5|          NULL|        NULL|      0.0|           80|            0|     115920384|ROAD_CLOSED_EVENT|   Perbaikan  ~ Akhi...|           2023-11-19 10:27:...|     2023-11-19 10:20:00|2023-11-19 10:29:...|2023-11-19 10:49:...|     ID|            Depok|         Krukut Raya|\n",
      "|1835158694|NONE|    5|       5|          NULL|        NULL|      0.0|          101|            0|    1845066035|ROAD_CLOSED_EVENT|                   NULL|           2023-10-12 13:05:...|     2023-10-12 13:05:00|2023-10-13 04:38:...|2023-11-19 10:49:...|     ID|    Jakarta Pusat|          Gajah Mada|\n",
      "|  69764658|NONE|    5|       5|          NULL|        NULL|      0.0|           10|            0|     108542573|ROAD_CLOSED_EVENT|              Di tutup |           2023-10-26 17:03:...|     2023-10-26 17:00:00|2023-11-07 05:50:...|2023-11-19 10:49:...|     ID|  Jakarta Selatan|                NULL|\n",
      "|1820907922|NONE|    5|       5|          NULL|        NULL|      0.0|           82|            0|     115919212|ROAD_CLOSED_EVENT|   Perbaikan  ~ Akhi...|           2023-11-19 10:27:...|     2023-11-19 10:20:00|2023-11-19 10:29:...|2023-11-19 10:49:...|     ID|            Depok|         Krukut Raya|\n",
      "|1819042078|NONE|    5|       5|         Depok|       Depok|      0.0|          292|            0|    1725640876|ROAD_CLOSED_EVENT|                   NULL|           2023-10-01 06:51:...|     2023-10-01 06:51:00|2023-11-19 09:20:...|2023-11-19 10:49:...|     ID|            Depok| Tol Cinere-Jagorawi|\n",
      "|1820937298|NONE|    5|       5|          NULL|        NULL|      0.0|           16|            0|     115919913|ROAD_CLOSED_EVENT|   Perbaikan  ~ Akhi...|           2023-11-19 10:27:...|     2023-11-19 10:20:00|2023-11-19 10:29:...|2023-11-19 10:49:...|     ID|            Depok|         Krukut Raya|\n",
      "| 635247766|NONE|    5|       5|          NULL|        NULL|      0.0|           75|            0|    1676634618|ROAD_CLOSED_EVENT|   Perbaikan Jembata...|           2023-09-03 14:31:...|     2023-09-03 14:31:00|2023-11-10 21:59:...|2023-11-19 10:49:...|     ID|Tangerang Selatan|          Ceger Raya|\n",
      "|1819079816|NONE|    5|       5|          NULL|        NULL|      0.0|          604|            0|      15885565|ROAD_CLOSED_EVENT|                   NULL|           2023-11-11 08:41:...|     2023-11-11 08:40:00|2023-11-19 09:20:...|2023-11-19 10:49:...|     ID|            Depok|                NULL|\n",
      "|1819034238|NONE|    5|       5|          NULL|        NULL|      0.0|          210|            0|      90461108|ROAD_CLOSED_EVENT|     Perbaikan ~ 12 Des|           2023-11-17 10:44:...|     2023-11-17 10:26:00|2023-11-19 09:20:...|2023-11-19 10:49:...|     ID|           Bekasi|         Pangkalan 1|\n",
      "+----------+----+-----+--------+--------------+------------+---------+-------------+-------------+--------------+-----------------+-----------------------+-------------------------------+------------------------+--------------------+--------------------+-------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, when, lit, lag, lead\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def replace_outliers_with_mean_adjacent(df, max_values):\n",
    "    '''handle distinguish value by replace with adjacent mean'''\n",
    "    windowSpec = Window.orderBy('timestamp_utc')  # Define your timestamp column here\n",
    "\n",
    "    for col_name in max_values.keys():\n",
    "        max_val = max_values.get(col_name, float('inf'))\n",
    "        df = df.withColumn(col_name,\n",
    "                           when((col(col_name) < 0) | (col(col_name) > max_val),\n",
    "                                  (lag(col_name, default=0).over(windowSpec)+ lead(col_name, default=0).over(windowSpec)) / 2.0)\n",
    "                           .otherwise(col(col_name)))\n",
    "\n",
    "    return df\n",
    "\n",
    "def consistent_dtype(df, dtype):\n",
    "    '''make sure the dtype is correct'''\n",
    "    for col_name in dtype.keys():\n",
    "        type = dtype.get(col_name)\n",
    "        df = df.withColumn(col_name, col(col_name).cast(type))\n",
    "    return df\n",
    "def saveToCSV(df) :\n",
    "  try:\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.write.csv('/transform_data', header=True, mode='append')\n",
    "    print(f\"Data successfully saved\")\n",
    "  except IOError as e:\n",
    "    print(f\"Error while saving data: {e}\")\n",
    "\n",
    "# Transformation prcess start\n",
    "if __name__ == \"__main__\":\n",
    "  # Create a SparkSession\n",
    "  spark = SparkSession.builder.appName(\"CSV Reader\").getOrCreate()\n",
    "\n",
    "  # Read CSV file into a Spark DataFrame\n",
    "  file_path = \"../raw_data/traffic_jam_data.csv\"\n",
    "  df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "  # drop unecessary columns\n",
    "  columns_to_drop = ['line_coordinates', \n",
    "                   'start_location', \n",
    "                   'end_location', \n",
    "                   'block_alert_id', \n",
    "                   'block_alert_type',\n",
    "                   'block_alert_description',\n",
    "                   'block_alert_update_datetime_utc',\n",
    "                   'block_start_datetime_utc',\n",
    "                   'update_datetime_utc']\n",
    "  columns_to_drop = ['line_coordinates']\n",
    "  df = df.drop(*columns_to_drop)\n",
    "\n",
    "  # rename publish_datetime_utc to timestamp_utc\n",
    "  df = df.withColumnRenamed(\"publish_datetime_utc\", \"timestamp_utc\")\n",
    "\n",
    "  # drom row with all missing value\n",
    "  df = df.dropna(how ='all')\n",
    "\n",
    "  # drop duplicates\n",
    "  df = df.dropDuplicates()\n",
    "\n",
    "  # consistent dtypes\n",
    "  # dtypes = {\n",
    "  #   'jam_id': 'integer', \n",
    "  #   'type': 'string', \n",
    "  #   'level': 'integer',\n",
    "  #   'severity': 'integer', \n",
    "  #   'speed_kmh': 'double', \n",
    "  #   'length_meters': 'integer',\n",
    "  #   'delay_seconds': 'integer',\n",
    "  #   'timestamp_utc': 'timestamp',\n",
    "  #   'country': 'string',\n",
    "  #   'city': 'string',\n",
    "  #   'street': 'string'}\n",
    "  dtypes = {\n",
    "    'jam_id': 'integer', \n",
    "    'type': 'string', \n",
    "    'level': 'integer',\n",
    "    'severity': 'integer', \n",
    "    'speed_kmh': 'double', \n",
    "    'length_meters': 'integer',\n",
    "    'delay_seconds': 'integer',\n",
    "    'start_location': 'string', \n",
    "    'end_location': 'string', \n",
    "    'block_alert_id': 'integer', \n",
    "    'block_alert_type': 'string',\n",
    "    'block_alert_description' : 'string',\n",
    "    'block_alert_update_datetime_utc': 'timestamp',\n",
    "    'block_start_datetime_utc' :'timestamp',\n",
    "    'timestamp_utc': 'timestamp',\n",
    "    'country': 'string',\n",
    "    'city': 'string',\n",
    "    'street': 'string'}\n",
    "  df = consistent_dtype(df, dtypes)\n",
    "\n",
    "  # consistent datetime format\n",
    "  df = df.withColumn('timestamp_utc', to_timestamp('timestamp_utc', 'yyyy-MM-dd HH:mm:ss'))\n",
    "  df = df.withColumn('block_alert_update_datetime_utc', to_timestamp('block_alert_update_datetime_utc', 'yyyy-MM-dd HH:mm:ss'))\n",
    "  df = df.withColumn('block_start_datetime_utc', to_timestamp('block_start_datetime_utc', 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "  # handle Disgusting Values\n",
    "  max_val = {'co':150000.0, 'no2': 3840.0, 'o3': 1200.0, 'pm10': 600.0, 'pm25': 500.0, 'so2': 800.0}\n",
    "  # df = replace_outliers_with_mean_adjacent(df, max_val)\n",
    "\n",
    "  # save df to csv file\n",
    "  # df.printSchema()\n",
    "  df.filter(df['block_alert_id'] != 0).show()\n",
    "  # df.select('block_alert_update_datetime_utc').distinct().show()\n",
    "  \n",
    "\n",
    "# Still error when saaving csv files or should be transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-----+----+-----+----+-------------------+\n",
      "|aqi|   co| no2|   o3|pm10| pm25| so2|      timestamp_utc|\n",
      "+---+-----+----+-----+----+-----+----+-------------------+\n",
      "| 94|265.0| 8.7|113.7|47.0|32.33|37.7|2023-11-14 10:00:00|\n",
      "|103|339.0|13.3| 95.3|53.0|36.67|39.3|2023-11-14 11:00:00|\n",
      "|115|413.0|18.0| 77.0|59.0| 41.0|41.0|2023-11-14 12:00:00|\n",
      "|118|420.0|16.7| 72.7|60.7| 42.0|38.3|2023-11-14 13:00:00|\n",
      "|121|427.0|15.3| 68.3|62.3| 43.0|35.7|2023-11-14 14:00:00|\n",
      "|123|434.0|14.0| 64.0|64.0| 44.0|33.0|2023-11-14 15:00:00|\n",
      "|130|419.7|14.3| 62.3|67.7|46.67|38.7|2023-11-14 16:00:00|\n",
      "|138|405.3|14.7| 60.7|71.3|49.33|44.3|2023-11-14 17:00:00|\n",
      "|145|391.0|15.0| 59.0|75.0| 52.0|50.0|2023-11-14 18:00:00|\n",
      "|148|426.7|16.0| 53.0|76.0| 53.0|53.7|2023-11-14 19:00:00|\n",
      "|150|462.3|17.0| 47.0|77.0| 54.0|57.3|2023-11-14 20:00:00|\n",
      "|153|498.0|18.0| 41.0|78.0| 55.0|61.0|2023-11-14 21:00:00|\n",
      "|153|565.0|19.0| 50.0|81.7|57.33|63.3|2023-11-14 22:00:00|\n",
      "|156|632.0|20.0| 59.0|85.3|59.67|65.7|2023-11-14 23:00:00|\n",
      "|159|699.0|21.0| 68.0|89.0| 62.0|68.0|2023-11-15 00:00:00|\n",
      "|153|584.0|15.7|109.7|83.3| 58.0|75.3|2023-11-15 01:00:00|\n",
      "|150|469.0|10.3|151.3|77.7| 54.0|82.7|2023-11-15 02:00:00|\n",
      "|139|354.0| 5.0|193.0|72.0| 50.0|90.0|2023-11-15 03:00:00|\n",
      "|130|331.3| 4.3|196.7|67.3|46.67|81.3|2023-11-15 04:00:00|\n",
      "|121|308.7| 3.7|200.3|62.7|43.33|72.7|2023-11-15 05:00:00|\n",
      "+---+-----+----+-----+----+-----+----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform Air Quality\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, when, lit, lag, lead\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def replace_outliers_with_mean_adjacent(df, max_values):\n",
    "    '''handle distinguish value by replace with adjacent mean'''\n",
    "    windowSpec = Window.orderBy('timestamp_utc')  # Define your timestamp column here\n",
    "\n",
    "    for col_name in max_values.keys():\n",
    "        max_val = max_values.get(col_name, float('inf'))\n",
    "        df = df.withColumn(col_name,\n",
    "                           when((col(col_name) < 0) | (col(col_name) > max_val),\n",
    "                                  (lag(col_name, default=0).over(windowSpec)+ lead(col_name, default=0).over(windowSpec)) / 2.0)\n",
    "                           .otherwise(col(col_name)))\n",
    "\n",
    "    return df\n",
    "\n",
    "def consistent_dtype(df, dtype):\n",
    "    '''make sure the dtype is correct'''\n",
    "    for col_name in dtype.keys():\n",
    "        type = dtype.get(col_name)\n",
    "        df = df.withColumn(col_name, col(col_name).cast(type))\n",
    "    return df\n",
    "def saveToCSV(df) :\n",
    "  try:\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.write.csv('/transform_data', header=True, mode='append')\n",
    "    print(f\"Data successfully saved\")\n",
    "  except IOError as e:\n",
    "    print(f\"Error while saving data: {e}\")\n",
    "\n",
    "# Transformation prcess start\n",
    "if __name__ == \"__main__\":\n",
    "  # Create a SparkSession\n",
    "  spark = SparkSession.builder.appName(\"CSV Reader\").getOrCreate()\n",
    "\n",
    "  # Read CSV file into a Spark DataFrame\n",
    "  file_path = \"../raw_data/air_quality_data.csv\"\n",
    "  df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "  # drop unecessary columns\n",
    "  columns_to_drop = ['datetime', 'timestamp_local', 'ts']\n",
    "  df = df.drop(*columns_to_drop)\n",
    "\n",
    "  # drom row with all missing value\n",
    "  df = df.dropna(how ='all')\n",
    "\n",
    "  # drop duplicates\n",
    "  df = df.dropDuplicates()\n",
    "\n",
    "  # consistent dtypes\n",
    "  dtypes = {'aqi': 'integer', 'co':'double', 'no2': 'double', 'o3': 'double', 'pm10': 'double', 'pm25': 'double', 'so2': 'double', 'timestamp_utc': 'timestamp'}\n",
    "  df = consistent_dtype(df, dtypes)\n",
    "\n",
    "  # consistent datetime format\n",
    "  df = df.withColumn('timestamp_utc', to_timestamp('timestamp_utc', 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "  # handle Disgusting Values\n",
    "  max_val = {'co':150000.0, 'no2': 3840.0, 'o3': 1200.0, 'pm10': 600.0, 'pm25': 500.0, 'so2': 800.0}\n",
    "  df = replace_outliers_with_mean_adjacent(df, max_val)\n",
    "\n",
    "  # save df to csv file\n",
    "  df.show()\n",
    "\n",
    "# Still error when saaving csv files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
